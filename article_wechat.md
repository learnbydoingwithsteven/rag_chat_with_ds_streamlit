# ğŸ“Š Building a Multilingual RAG Chatbot for Financial Data Analysis ğŸ“ˆ
# ğŸ“Š æ„å»ºå¤šè¯­è¨€ RAG èŠå¤©æœºå™¨äººç”¨äºè´¢åŠ¡æ•°æ®åˆ†æ ğŸ“ˆ

![Application Screenshot](assets/app_screenshot.png)

## ğŸ“ Introduction / ç®€ä»‹

In this article, I'll share how I built a multilingual Retrieval Augmented Generation (RAG) chatbot that specializes in Italian harmonized financial statements (Bilanci Armonizzati). This project combines natural language processing, vector databases, and large language models to create an intelligent assistant capable of answering complex financial questions in both English and Italian.

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘å°†åˆ†äº«å¦‚ä½•æ„å»ºä¸€ä¸ªä¸“æ³¨äºæ„å¤§åˆ©åè°ƒè´¢åŠ¡æŠ¥è¡¨ï¼ˆBilanci Armonizzatiï¼‰çš„å¤šè¯­è¨€æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰èŠå¤©æœºå™¨äººã€‚è¯¥é¡¹ç›®ç»“åˆäº†è‡ªç„¶è¯­è¨€å¤„ç†ã€å‘é‡æ•°æ®åº“å’Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œåˆ›å»ºäº†ä¸€ä¸ªèƒ½å¤Ÿç”¨è‹±è¯­å’Œæ„å¤§åˆ©è¯­å›ç­”å¤æ‚è´¢åŠ¡é—®é¢˜çš„æ™ºèƒ½åŠ©æ‰‹ã€‚

## ğŸ¤” What is RAG? / ä»€ä¹ˆæ˜¯ RAGï¼Ÿ

RAG (Retrieval Augmented Generation) is a powerful technique that enhances language models by first retrieving relevant information from a knowledge base, then using that information to generate accurate, contextual responses. Unlike traditional chatbots that rely solely on pre-trained knowledge, RAG systems can access up-to-date, domain-specific information, significantly improving response quality, accuracy, and relevance.

RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æ˜¯ä¸€ç§å¼ºå¤§çš„æŠ€æœ¯ï¼Œå®ƒé€šè¿‡é¦–å…ˆä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯ï¼Œç„¶åä½¿ç”¨è¯¥ä¿¡æ¯ç”Ÿæˆå‡†ç¡®ã€å…·æœ‰ä¸Šä¸‹æ–‡çš„å“åº”æ¥å¢å¼ºè¯­è¨€æ¨¡å‹ã€‚ä¸ä»…ä¾èµ–é¢„è®­ç»ƒçŸ¥è¯†çš„ä¼ ç»ŸèŠå¤©æœºå™¨äººä¸åŒï¼ŒRAG ç³»ç»Ÿå¯ä»¥è®¿é—®æœ€æ–°çš„ã€ç‰¹å®šé¢†åŸŸçš„ä¿¡æ¯ï¼Œæ˜¾è‘—æé«˜å“åº”è´¨é‡ã€å‡†ç¡®æ€§å’Œç›¸å…³æ€§ã€‚

## âœ¨ Core Features / æ ¸å¿ƒåŠŸèƒ½

### ğŸŒ Multilingual Support / å¤šè¯­è¨€æ”¯æŒ

Our chatbot fully supports both English and Italian, making it accessible to a wider audience. The user interface, responses, and even document processing capabilities work seamlessly in both languages.

æˆ‘ä»¬çš„èŠå¤©æœºå™¨äººå®Œå…¨æ”¯æŒè‹±è¯­å’Œæ„å¤§åˆ©è¯­ï¼Œä½¿å…¶å¯¹æ›´å¹¿æ³›çš„å—ä¼—å¯è®¿é—®ã€‚ç”¨æˆ·ç•Œé¢ã€å“åº”ç”šè‡³æ–‡æ¡£å¤„ç†åŠŸèƒ½åœ¨ä¸¤ç§è¯­è¨€ä¸­éƒ½èƒ½æ— ç¼å·¥ä½œã€‚

### ğŸ§  Advanced RAG Architecture / å…ˆè¿›çš„ RAG æ¶æ„

The system uses sentence transformers to create vector embeddings of document chunks. When a user asks a question, it's also converted to a vector and compared against the document embeddings to find the most relevant information. This retrieved context is then sent to the LLM along with the original query to generate an informed response.

è¯¥ç³»ç»Ÿä½¿ç”¨å¥å­è½¬æ¢å™¨ä¸ºæ–‡æ¡£å—åˆ›å»ºå‘é‡åµŒå…¥ã€‚å½“ç”¨æˆ·æå‡ºé—®é¢˜æ—¶ï¼Œå®ƒä¹Ÿä¼šè¢«è½¬æ¢ä¸ºå‘é‡ï¼Œå¹¶ä¸æ–‡æ¡£åµŒå…¥è¿›è¡Œæ¯”è¾ƒï¼Œä»¥æ‰¾åˆ°æœ€ç›¸å…³çš„ä¿¡æ¯ã€‚ç„¶åå°†è¿™ä¸ªæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¸åŸå§‹æŸ¥è¯¢ä¸€èµ·å‘é€åˆ° LLMï¼Œä»¥ç”Ÿæˆæœ‰æ ¹æ®çš„å“åº”ã€‚

```python
# Example of how RAG works in our system:
def get_relevant_context(query, top_k=5):
    # Convert query to vector
    query_embedding = embeddings_model.encode(query)
    
    # Search vector database for similar documents
    context_chunks = vector_db.similarity_search_by_vector(
        query_embedding, k=top_k
    )
    
    # Compile retrieved contexts
    context = "\n\n".join([chunk.page_content for chunk in context_chunks])
    sources = [chunk.metadata["source"] for chunk in context_chunks]
    
    return context, sources
```

### ğŸ’¡ Multiple LLM Options / å¤šä¸ª LLM é€‰é¡¹

The application supports both cloud-based and local LLMs:
- **Groq API**: Access to powerful models like Llama 3.1 and Gemma2
- **Local Ollama**: Run open-source models locally for privacy and cost savings

åº”ç”¨ç¨‹åºæ”¯æŒåŸºäºäº‘çš„å’Œæœ¬åœ°çš„ LLMï¼š
- **Groq API**ï¼šè®¿é—®å¼ºå¤§çš„æ¨¡å‹ï¼Œå¦‚ Llama 3.1 å’Œ Gemma2
- **æœ¬åœ° Ollama**ï¼šåœ¨æœ¬åœ°è¿è¡Œå¼€æºæ¨¡å‹ï¼Œä»¥ä¿æŠ¤éšç§å’ŒèŠ‚çœæˆæœ¬

### ğŸ¨ Vector Visualization / å‘é‡å¯è§†åŒ–

One of the most fascinating features is the ability to visualize document embeddings in 2D space. Users can choose between PCA, t-SNE, or UMAP dimensionality reduction techniques. Each document source is color-coded for easy identification, allowing users to see how similar documents cluster together.

æœ€å¼•äººå…¥èƒœçš„åŠŸèƒ½ä¹‹ä¸€æ˜¯èƒ½å¤Ÿåœ¨ 2D ç©ºé—´ä¸­å¯è§†åŒ–æ–‡æ¡£åµŒå…¥ã€‚ç”¨æˆ·å¯ä»¥åœ¨ PCAã€t-SNE æˆ– UMAP é™ç»´æŠ€æœ¯ä¹‹é—´è¿›è¡Œé€‰æ‹©ã€‚æ¯ä¸ªæ–‡æ¡£æºéƒ½æœ‰é¢œè‰²ç¼–ç ï¼Œä¾¿äºè¯†åˆ«ï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿçœ‹åˆ°ç›¸ä¼¼æ–‡æ¡£å¦‚ä½•èšé›†åœ¨ä¸€èµ·ã€‚

### ğŸ‡®ğŸ‡¹ BDAP API Integration / BDAP API é›†æˆ

The application integrates with the Italian BDAP (Banca Dati delle Amministrazioni Pubbliche) APIs to access official financial data for Italian local governments. This allows users to query historical financial information, visualize trends, and analyze structural patterns in government spending.

è¯¥åº”ç”¨ç¨‹åºä¸æ„å¤§åˆ© BDAPï¼ˆå…¬å…±è¡Œæ”¿æ•°æ®åº“ï¼‰API é›†æˆï¼Œä»¥è®¿é—®æ„å¤§åˆ©åœ°æ–¹æ”¿åºœçš„å®˜æ–¹è´¢åŠ¡æ•°æ®ã€‚è¿™ä½¿ç”¨æˆ·èƒ½å¤ŸæŸ¥è¯¢å†å²è´¢åŠ¡ä¿¡æ¯ï¼Œå¯è§†åŒ–è¶‹åŠ¿ï¼Œå¹¶åˆ†ææ”¿åºœæ”¯å‡ºçš„ç»“æ„æ¨¡å¼ã€‚

## ğŸ› ï¸ Technical Implementation / æŠ€æœ¯å®ç°

### ğŸ“„ Document Processing Pipeline / æ–‡æ¡£å¤„ç†æµç¨‹

The system includes a complete pipeline for processing documents:

1. **PDF to Text Conversion**: Extracts text from PDF documents
2. **Text Chunking**: Splits text into manageable chunks with appropriate overlap
3. **Vector Embedding**: Converts text chunks into vector embeddings
4. **Database Storage**: Stores vectors in a SQLite database for efficient retrieval

ç³»ç»ŸåŒ…æ‹¬å®Œæ•´çš„æ–‡æ¡£å¤„ç†æµç¨‹ï¼š

1. **PDF åˆ°æ–‡æœ¬è½¬æ¢**ï¼šä» PDF æ–‡æ¡£ä¸­æå–æ–‡æœ¬
2. **æ–‡æœ¬åˆ†å—**ï¼šå°†æ–‡æœ¬åˆ†å‰²æˆå…·æœ‰é€‚å½“é‡å çš„å¯ç®¡ç†å—
3. **å‘é‡åµŒå…¥**ï¼šå°†æ–‡æœ¬å—è½¬æ¢ä¸ºå‘é‡åµŒå…¥
4. **æ•°æ®åº“å­˜å‚¨**ï¼šå°†å‘é‡å­˜å‚¨åœ¨ SQLite æ•°æ®åº“ä¸­ï¼Œä»¥ä¾¿é«˜æ•ˆæ£€ç´¢

```python
def process_text_files():
    # Find all text files
    text_files = list(text_dir.glob("*.txt"))
    
    # Process each text file
    for text_file in text_files:
        # Read text file
        with open(text_file, 'r', encoding='utf-8') as f:
            text = f.read()
        
        # Split text into chunks
        chunks = split_text_into_chunks(text, text_file.stem)
        
        # Generate embeddings for all chunks
        embeddings = get_embeddings(chunks, model)
        
        # Store chunks and embeddings in database
        store_in_database(chunks, embeddings)
```

### ğŸ–¥ï¸ Streamlit Interface / Streamlit ç•Œé¢

The application uses Streamlit to create an interactive web interface with multiple tabs:
- **RAG Chat**: The main chat interface
- **Documents**: Upload and manage PDF documents
- **Vector Visualizations**: Interactive 2D visualization of document embeddings
- **Financial Data**: Access to BDAP financial data with visualization tools
- **RAG Settings**: Configuration for chunk size, overlap, and other parameters

åº”ç”¨ç¨‹åºä½¿ç”¨ Streamlit åˆ›å»ºå…·æœ‰å¤šä¸ªé€‰é¡¹å¡çš„äº¤äº’å¼ Web ç•Œé¢ï¼š
- **RAG èŠå¤©**ï¼šä¸»è¦èŠå¤©ç•Œé¢
- **æ–‡æ¡£**ï¼šä¸Šä¼ å’Œç®¡ç† PDF æ–‡æ¡£
- **å‘é‡å¯è§†åŒ–**ï¼šæ–‡æ¡£åµŒå…¥çš„äº¤äº’å¼ 2D å¯è§†åŒ–
- **è´¢åŠ¡æ•°æ®**ï¼šè®¿é—® BDAP è´¢åŠ¡æ•°æ®ï¼Œå¹¶æä¾›å¯è§†åŒ–å·¥å…·
- **RAG è®¾ç½®**ï¼šåˆ†å—å¤§å°ã€é‡å å’Œå…¶ä»–å‚æ•°çš„é…ç½®

### ğŸ” Web Search Integration / ç½‘ç»œæœç´¢é›†æˆ

To supplement the internal knowledge base, the application can perform web searches using DuckDuckGo. This allows it to provide up-to-date information even on topics that may not be covered in the document database.

ä¸ºäº†è¡¥å……å†…éƒ¨çŸ¥è¯†åº“ï¼Œåº”ç”¨ç¨‹åºå¯ä»¥ä½¿ç”¨ DuckDuckGo æ‰§è¡Œç½‘ç»œæœç´¢ã€‚è¿™ä½¿å®ƒèƒ½å¤Ÿæä¾›æœ€æ–°ä¿¡æ¯ï¼Œå³ä½¿æ˜¯åœ¨æ–‡æ¡£æ•°æ®åº“ä¸­å¯èƒ½æœªæ¶µç›–çš„ä¸»é¢˜ã€‚

## âœ… Benefits of This Approach / è¿™ç§æ–¹æ³•çš„å¥½å¤„

1. **Accuracy**: By retrieving relevant context before generating responses, the chatbot provides more accurate and contextually appropriate answers
2. **Transparency**: Users can see the sources used to generate responses
3. **Customization**: The knowledge base can be easily expanded by adding more documents
4. **Multilingual Capability**: Serve diverse user populations with the same system
5. **Reduced Hallucinations**: The RAG approach significantly reduces the problem of AI hallucinations by grounding responses in retrieved documents

1. **å‡†ç¡®æ€§**ï¼šé€šè¿‡åœ¨ç”Ÿæˆå“åº”ä¹‹å‰æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡ï¼ŒèŠå¤©æœºå™¨äººæä¾›æ›´å‡†ç¡®å’Œä¸Šä¸‹æ–‡é€‚å½“çš„ç­”æ¡ˆ
2. **é€æ˜åº¦**ï¼šç”¨æˆ·å¯ä»¥çœ‹åˆ°ç”¨äºç”Ÿæˆå“åº”çš„æ¥æº
3. **å®šåˆ¶åŒ–**ï¼šå¯ä»¥é€šè¿‡æ·»åŠ æ›´å¤šæ–‡æ¡£è½»æ¾æ‰©å±•çŸ¥è¯†åº“
4. **å¤šè¯­è¨€èƒ½åŠ›**ï¼šç”¨åŒä¸€ä¸ªç³»ç»ŸæœåŠ¡ä¸åŒçš„ç”¨æˆ·ç¾¤ä½“
5. **å‡å°‘å¹»è§‰**ï¼šRAG æ–¹æ³•é€šè¿‡å°†å“åº”å»ºç«‹åœ¨æ£€ç´¢åˆ°çš„æ–‡æ¡£åŸºç¡€ä¸Šï¼Œæ˜¾è‘—å‡å°‘äº† AI å¹»è§‰é—®é¢˜

## ğŸš§ Challenges and Solutions / æŒ‘æˆ˜ä¸è§£å†³æ–¹æ¡ˆ

### ğŸ’¾ Vector Database Schema / å‘é‡æ•°æ®åº“æ¨¡å¼

One challenge was ensuring compatibility between the database schema used for storing embeddings and the schema expected by the visualization components. We implemented a flexible approach that checks the database structure and adapts accordingly.

ä¸€ä¸ªæŒ‘æˆ˜æ˜¯ç¡®ä¿ç”¨äºå­˜å‚¨åµŒå…¥çš„æ•°æ®åº“æ¨¡å¼ä¸å¯è§†åŒ–ç»„ä»¶æœŸæœ›çš„æ¨¡å¼ä¹‹é—´çš„å…¼å®¹æ€§ã€‚æˆ‘ä»¬å®ç°äº†ä¸€ç§çµæ´»çš„æ–¹æ³•ï¼Œæ£€æŸ¥æ•°æ®åº“ç»“æ„å¹¶ç›¸åº”åœ°è¿›è¡Œè°ƒæ•´ã€‚

### ğŸ”— API Reliability / API å¯é æ€§

The BDAP APIs occasionally experience timeouts or temporary unavailability. We implemented robust retry mechanisms with appropriate error handling to ensure a smooth user experience even when API issues occur.

BDAP API å¶å°”ä¼šé‡åˆ°è¶…æ—¶æˆ–ä¸´æ—¶ä¸å¯ç”¨çš„æƒ…å†µã€‚æˆ‘ä»¬å®ç°äº†å…·æœ‰é€‚å½“é”™è¯¯å¤„ç†çš„å¼ºå¤§é‡è¯•æœºåˆ¶ï¼Œä»¥ç¡®ä¿å³ä½¿åœ¨å‘ç”Ÿ API é—®é¢˜æ—¶ä¹Ÿèƒ½æä¾›æµç•…çš„ç”¨æˆ·ä½“éªŒã€‚

### ğŸš€ Visualization Performance / å¯è§†åŒ–æ€§èƒ½

Visualizing large numbers of embeddings can be computationally intensive. We implemented sampling and pagination strategies to maintain performance while still providing useful insights.

å¯è§†åŒ–å¤§é‡åµŒå…¥å¯èƒ½ä¼šæ¶ˆè€—å¤§é‡è®¡ç®—èµ„æºã€‚æˆ‘ä»¬å®ç°äº†é‡‡æ ·å’Œåˆ†é¡µç­–ç•¥ï¼Œä»¥åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶ä»æä¾›æœ‰ç”¨çš„è§è§£ã€‚

## ğŸ Conclusion / ç»“è®º

This multilingual RAG chatbot demonstrates the power of combining vector databases, LLMs, and domain-specific data to create intelligent applications. The ability to ground responses in retrieved documents significantly improves accuracy and trustworthiness compared to traditional chatbots.

è¿™ä¸ªå¤šè¯­è¨€ RAG èŠå¤©æœºå™¨äººå±•ç¤ºäº†ç»“åˆå‘é‡æ•°æ®åº“ã€LLM å’Œç‰¹å®šé¢†åŸŸæ•°æ®åˆ›å»ºæ™ºèƒ½åº”ç”¨ç¨‹åºçš„å¼ºå¤§åŠŸèƒ½ã€‚ä¸ä¼ ç»ŸèŠå¤©æœºå™¨äººç›¸æ¯”ï¼Œå°†å“åº”å»ºç«‹åœ¨æ£€ç´¢åˆ°çš„æ–‡æ¡£åŸºç¡€ä¸Šçš„èƒ½åŠ›æ˜¾è‘—æé«˜äº†å‡†ç¡®æ€§å’Œå¯ä¿¡åº¦ã€‚

The project is open-source and available at: [https://github.com/learnbydoingwithsteven/rag_chat_with_ds_streamlit](https://github.com/learnbydoingwithsteven/rag_chat_with_ds_streamlit)

è¯¥é¡¹ç›®æ˜¯å¼€æºçš„ï¼Œå¯åœ¨ä»¥ä¸‹ç½‘å€è·å–ï¼š[https://github.com/learnbydoingwithsteven/rag_chat_with_ds_streamlit](https://github.com/learnbydoingwithsteven/rag_chat_with_ds_streamlit)

---

### About the Author / å…³äºä½œè€…

I'm a data scientist and machine learning engineer passionate about building AI applications that solve real-world problems. This project combines my interests in natural language processing, financial data analysis, and multilingual applications.

æˆ‘æ˜¯ä¸€åæ•°æ®ç§‘å­¦å®¶å’Œæœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆï¼Œçƒ­è¡·äºæ„å»ºè§£å†³ç°å®ä¸–ç•Œé—®é¢˜çš„ AI åº”ç”¨ç¨‹åºã€‚è¿™ä¸ªé¡¹ç›®ç»“åˆäº†æˆ‘å¯¹è‡ªç„¶è¯­è¨€å¤„ç†ã€è´¢åŠ¡æ•°æ®åˆ†æå’Œå¤šè¯­è¨€åº”ç”¨ç¨‹åºçš„å…´è¶£ã€‚

Feel free to connect with me on GitHub or leave comments below!

## ğŸ“œ License / è®¸å¯è¯

This project is licensed under the Apache License 2.0. See the [LICENSE](https://github.com/learnbydoingwithsteven/rag_chat_with_ds_streamlit/blob/main/LICENSE) file in the repository for full details.

è¯¥é¡¹ç›®æ ¹æ® Apache è®¸å¯è¯ 2.0 æˆæƒã€‚æœ‰å…³å®Œæ•´è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…å­˜å‚¨åº“ä¸­çš„ [LICENSE](https://github.com/learnbydoingwithsteven/rag_chat_with_ds_streamlit/blob/main/LICENSE) æ–‡ä»¶ã€‚

æ¬¢è¿åœ¨ GitHub ä¸Šä¸æˆ‘è”ç³»æˆ–åœ¨ä¸‹æ–¹ç•™è¨€ï¼
